{
  "base_model_path": "Qwen3-1.7B",
  "weights_base_path": "linear-attention-checkpoints",
  "description": "Top 50% - GLA layers: [3, 7, 8, 9, 10], RWKV7 layers: [17, 18, 19, 20, 21, 22]",
  "layer_attention_types": [
    "full_attention",
    "full_attention",
    "gla",
    "full_attention",
    "full_attention",
    "full_attention",
    "gla",
    "gla",
    "gla",
    "gla",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "rwkv7",
    "rwkv7",
    "rwkv7",
    "rwkv7",
    "rwkv7",
    "rwkv7",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "rwkv7_config_path": "configs/linear_attn_configs/rwkv7_config.json",
  "gla_config_path": "configs/linear_attn_configs/gla_config.json"
}