<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      .content-margin-container {
        display: flex;
        width: 100%;
        justify-content: left;
        align-items: center;
      }
      .main-content-block {
        width: 70%;
        max-width: 1100px;
        background-color: #fff;
        border-left: 1px solid #ddd;
        border-right: 1px solid #ddd;
        padding: 8px 8px 8px 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        line-height: 1.6;
      }
      .margin-left-block {
        font-size: 14px;
        width: 15%;
        max-width: 130px;
        position: relative;
        margin-left: 10px;
        text-align: left;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        padding: 5px;
      }
      .margin-right-block {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-size: 14px;
        width: 25%;
        max-width: 256px;
        position: relative;
        text-align: left;
        padding: 10px;
        color: #666;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: auto;
      }

      a:link,
      a:visited {
        color: #0e7862;
        text-decoration: none;
      }
      a:hover {
        color: #24b597;
      }

      h1 {
        font-size: 22px;
        margin-top: 20px;
        margin-bottom: 10px;
        color: #333;
      }

      h2 {
        font-size: 18px;
        margin-top: 16px;
        margin-bottom: 8px;
        color: #444;
      }

      table.header {
        font-weight: 300;
        font-size: 17px;
        flex-grow: 1;
        width: 70%;
        max-width: calc(100% - 290px);
      }
      table td,
      table td * {
        vertical-align: middle;
        position: relative;
      }

      hr {
        height: 1px;
        border: none;
        background-color: #ddd;
      }

      div.hypothesis {
        width: 90%;
        background-color: #f0f7f5;
        border: 1px solid #0e7862;
        border-radius: 8px;
        font-family: Georgia, serif;
        font-size: 17px;
        font-style: italic;
        text-align: center;
        margin: 20px auto;
        padding: 16px 20px;
      }

      div.citation {
        font-size: 0.85em;
        background-color: #fafafa;
        padding: 15px;
        border-radius: 4px;
      }

      code {
        background-color: #f4f4f4;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: "Courier New", monospace;
        font-size: 0.9em;
      }

      .figure-caption {
        font-size: 13px;
        color: #666;
        text-align: center;
        margin-top: 8px;
        font-style: italic;
      }

      table.results {
        border-collapse: collapse;
        width: 100%;
        margin: 15px 0;
      }
      table.results th,
      table.results td {
        border: 1px solid #ddd;
        padding: 10px;
        text-align: center;
      }
      table.results th {
        background-color: #f0f7f5;
        font-weight: 600;
      }
      table.results tr:nth-child(even) {
        background-color: #fafafa;
      }

      .figure-placeholder {
        background-color: #f0f0f0;
        height: 300px;
        display: flex;
        align-items: center;
        justify-content: center;
        margin: 20px 0;
        border: 1px dashed #999;
        color: #666;
        font-style: italic;
      }
    </style>

    <title>Compute-Aware Hybrid Attention Architecture Search</title>
    <meta
      property="og:title"
      content="Compute-Aware Hybrid Attention Architecture Search"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span style="font-size: 28px; font-family: Georgia, serif">
                Compute-Aware Hybrid Attention Architecture Search:<br />
                <span style="font-size: 22px; color: #666">
                  Selective Linear Attention Replacement via Layer-wise
                  Knowledge Distillation
                </span>
              </span>
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Author 1</a></span>
            </td>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Author 2</a></span>
            </td>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Author 3</a></span>
            </td>
          </tr>
          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 16px"
                >Final project for 6.7960, MIT &nbsp;|&nbsp; December 2025</span
              >
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#related">Related Work</a><br /><br />
          <a href="#linear-attention">Gated Linear Attention</a><br /><br />
          <a href="#stage0">Stage 0 Training</a><br /><br />
          <a href="#distillation">Knowledge Distillation</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#limitations">Limitations</a><br /><br />
          <a href="#references">References</a><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <h1>Introduction and Motivation</h1>
        <p>
          The computational demands of large language models continue to grow,
          driven largely by the quadratic complexity of self-attention. For a
          sequence of length \(n\), standard attention requires \(O(n^2)\) time
          and memory. This makes long-context inference expensive and limits
          practical deployment.
        </p>

        <p>
          Linear attention mechanisms reduce this complexity to \(O(n)\), but
          replacing all attention layers with linear variants typically degrades
          model quality. This raises a question: are all attention layers
          equally important, or can we selectively replace only those layers
          where linear attention performs well?
        </p>

        <p>
          Our goal was to develop a method for identifying which layers in a
          transformer can be safely replaced with linear attention, and which
          layers should retain full softmax attention.
        </p>

        <div class="hypothesis">
          <strong>Hypothesis:</strong> Transformer layers exhibit varying
          sensitivity to attention mechanism replacement. By measuring how well
          a linear attention module can replicate each layer's behavior through
          distillation loss, we can identify an optimal hybrid architecture that
          improves efficiency while preserving model quality.
        </div>

        <p>
          We developed a two-stage approach. In Stage 0, we train linear
          attention replacements for each layer independently using knowledge
          distillation. This gives us a measure of how "replaceable" each layer
          is. In Stage 1, we assemble a hybrid model by selecting the
          best-performing layers for replacement and fine-tune the complete
          model using knowledge distillation with a decoupled top-k KL
          divergence objective.
        </p>

        <p>
          We conducted experiments using Qwen3-1.7B (28 layers, 2048 hidden
          size) as the student model architecture and Qwen3-8B as the teacher
          for Stage 1 distillation. We evaluated Gated Linear Attention (GLA)
          from the Flash Linear Attention library. Our results show clear
          patterns in layer sensitivity, with middle layers being significantly
          harder to replace than early or late layers.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br />
        The quadratic scaling becomes particularly problematic for applications
        like document understanding and multi-turn dialogue that require long
        context windows.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Our approach treats architecture search as a measurement problem rather
        than a search problem.
      </div>
    </div>

    <div class="content-margin-container" id="related">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Related Work</h1>

        <h2>Knowledge Distillation for Linear Attention</h2>
        <p>
          Recent work has demonstrated that linear attention models can be
          trained effectively through knowledge distillation from transformer
          teachers. The NVIDIA Nemotron project <a href="#ref_1">[1]</a> showed
          that distillation can transfer knowledge from large teacher models to
          more efficient student architectures, achieving competitive
          performance without full pretraining costs.
        </p>

        <p>
          The RAD-LM work <a href="#ref_2">[2]</a> specifically focused on
          linearizing pretrained language models through distillation. They
          demonstrated that by carefully selecting which layers to convert to
          linear attention and using appropriate distillation objectives, one
          can preserve most of the original model's capabilities while gaining
          efficiency benefits.
        </p>

        <p>
          The Liquid Foundation Models (LFM) work
          <a href="#ref_3">[3]</a> introduced a decoupled knowledge distillation
          approach that separates the distillation loss into components for the
          top-k tokens and the remaining probability mass. This approach
          addresses the challenge that standard KL divergence over large
          vocabularies is dominated by low-probability tokens that may not be
          meaningful for knowledge transfer.
        </p>

        <p>
          Our work builds on these knowledge distillation techniques for
          transferring capabilities between architectures. The Nemotron and
          RAD-LM projects showed that distillation can effectively transfer
          knowledge from transformer teachers to linear attention students. The
          Liquid Foundation Models work provided the decoupled top-k KL
          divergence objective that we adopt for our Stage 1 training.
        </p>

        <p>
          Our contribution extends this line of research by using distillation
          not just for training, but as a measurement tool for architecture
          search. By distilling each layer independently, we can quantify
          layer-wise sensitivity and make informed decisions about which layers
          to replace.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The key insight from these works is that not all layers need full
        attention, and distillation can effectively transfer the necessary
        capabilities.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Unlike prior approaches that use fixed patterns for layer assignment,
        our method learns which layers should use which attention type based on
        empirical distillation results.
      </div>
    </div>

    <div class="content-margin-container" id="linear-attention">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Linear Attention Mechanisms</h1>

        <p>
          We investigated two linear attention mechanisms as candidates for
          layer replacement: Gated Linear Attention (GLA) and RWKV7.
        </p>

        <h2>Gated Linear Attention (GLA)</h2>
        <p>
          Gated Linear Attention (GLA) <a href="#ref_4">[4]</a> replaces softmax
          attention with a gated recurrent formulation. The key insight is that
          attention can be computed as a weighted sum over a recurrent state,
          with data-dependent gates controlling information flow.
        </p>

        <p>
          For each position \(t\), GLA maintains a state matrix \(S_t\) and
          computes:
        </p>
        <p style="text-align: center">
          \[S_t = G_t \odot S_{t-1} + k_t^T v_t\] \[o_t = q_t S_t\]
        </p>
        <p>
          where \(G_t\) is a learned gate matrix that controls how much of the
          previous state to retain. This gating mechanism provides expressivity
          beyond simple linear attention by allowing the model to selectively
          forget or retain information from previous positions.
        </p>

        <p>
          The recurrent formulation enables \(O(n)\) complexity since each
          position only needs to update and query a fixed-size state matrix. GLA
          also supports efficient parallel training through a chunked
          computation that processes multiple positions simultaneously.
        </p>

        <h2>RWKV7</h2>
        <p>
          RWKV7 <a href="#ref_7">[7]</a> is a linear attention variant that
          combines aspects of RNNs and transformers. It uses a time-mixing
          mechanism with learned decay rates to control information flow across
          positions. RWKV7 maintains a recurrent state that is updated at each
          position:
        </p>
        <p style="text-align: center">
          \[s_t = \text{diag}(w) \cdot s_{t-1} + k_t^T v_t\] \[o_t = \sigma(r_t)
          \odot (q_t \cdot s_t)\]
        </p>
        <p>
          where \(w\) is a learned decay vector, \(r_t\) is a receptance gate,
          and \(\sigma\) is a nonlinearity. The decay mechanism allows RWKV7 to
          model different time scales of information retention.
        </p>

        <p>
          We use implementations from the Flash Linear Attention (FLA) library,
          which provides optimized CUDA kernels for both GLA and RWKV7 for
          training and inference.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The key difference from standard attention is that these linear
        mechanisms never materialize the full \(n \times n\) attention matrix.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        GLA's gating mechanism makes it more expressive than simple linear
        attention while maintaining linear complexity.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        RWKV7's decay mechanism provides a different inductive bias, allowing
        explicit control over how quickly information fades.
      </div>
    </div>

    <div class="content-margin-container" id="stage0">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Stage 0: Layer-wise Linear Attention Training</h1>

        <p>
          The first stage trains linear attention replacements for each layer
          independently. We trained both GLA and RWKV7 variants to measure how
          well each layer's behavior can be approximated by linear attention,
          which tells us which layers are good candidates for replacement.
        </p>

        <h2>Training Setup</h2>
        <p>
          We modified the Qwen3 architecture to create a
          <code>Qwen3ForNAS</code> class that outputs the hidden states before
          and after any specified layer. For each layer \(l\), we train separate
          GLA and RWKV7 blocks to match the teacher's behavior by minimizing
          MSE:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{MSE}}^{(l)} = \mathbb{E}_{x \sim \mathcal{D}}
          \left[ \left\| f_{\text{student}}^{(l)}(h^{(l-1)}) -
          f_{\text{teacher}}^{(l)}(h^{(l-1)}) \right\|_2^2 \right]\]
        </p>

        <h2>Normalized Loss</h2>
        <p>
          Different layers have different activation magnitudes, making raw MSE
          values incomparable. We compute a normalized loss that measures the
          student's error relative to the magnitude of the teacher's update:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{norm}}^{(l)} =
          \frac{\text{MSE}(f_{\text{student}}^{(l)}(h^{(l-1)}),
          h^{(l)})}{\text{MSE}(h^{(l-1)}, h^{(l)})}\]
        </p>
        <p>
          A normalized loss of 0.0 means perfect reproduction; 1.0 means the
          student is no better than passing through the input unchanged.
        </p>

        <h2>Training Configuration</h2>
        <p>
          We trained each layer on approximately 40 million tokens from the
          DCLM-baseline-1.0 dataset with batch size 32, sequence length 1024,
          learning rate 1e-3 with cosine decay to 1e-5, and 10% warmup. Each
          layer takes approximately 2 hours to train on a single GPU. We trained
          GLA for layers 1-10 and RWKV7 for layers 11-22, based on preliminary
          experiments showing these ranges worked best for each architecture.
        </p>

        <h2>Layer-wise Training Results</h2>

        <div class="figure-placeholder">
          [FILL IN WITH GLA STUDENT LAYER TRAINING CURVES FIGURE]
        </div>
        <p class="figure-caption">
          Figure 1: Training curves for GLA student layers showing normalized
          MSE loss over training.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH RWKV7 STUDENT LAYER TRAINING CURVES FIGURE]
        </div>
        <p class="figure-caption">
          Figure 2: Training curves for RWKV7 student layers showing normalized
          MSE loss over training.
        </p>

        <h2>RWKV7 Issues</h2>
        <div class="figure-placeholder">
          [FILL IN WITH EXPLANATION OF RWKV7 ISSUES - WHY IT DIDN'T WORK FOR
          FINAL MODEL]
        </div>
        <p>
          (**TODO**) Due to these issues with RWKV7, we proceeded with only GLA
          layers for the final hybrid model configuration.
        </p>

        <h2>Identifying the Best Layers</h2>
        <p>
          After training, we ranked layers by their final normalized loss. Based
          on our experiments, we constrained GLA to layers 1-10. The results
          revealed a clear pattern: middle layers (roughly layers 8-14) have
          significantly higher normalized loss than early or late layers.
        </p>

        <p>
          This finding aligns with prior work on transformer interpretability.
          Clark et al. <a href="#ref_5">[5]</a> showed that early layers
          primarily detect local syntactic patterns while middle layers handle
          more complex semantic relationships. Tenney et al.
          <a href="#ref_6">[6]</a> demonstrated that the "classical NLP
          pipeline" emerges across layers, with syntax in early layers and
          semantics in middle layers. Our results suggest that the complex
          semantic operations in middle layers fundamentally require the
          expressivity of softmax attention, while the more local computations
          in early layers can be approximated by linear attention.
        </p>

        <table class="results">
          <tr>
            <th>Rank</th>
            <th>Layer</th>
            <th>Normalized Loss</th>
          </tr>
          <tr>
            <td>1</td>
            <td>3</td>
            <td>0.0003</td>
          </tr>
          <tr>
            <td>2</td>
            <td>9</td>
            <td>0.1059</td>
          </tr>
          <tr>
            <td>3</td>
            <td>10</td>
            <td>0.1186</td>
          </tr>
          <tr>
            <td>4</td>
            <td>7</td>
            <td>0.2129</td>
          </tr>
          <tr>
            <td>5</td>
            <td>8</td>
            <td>0.2396</td>
          </tr>
          <tr>
            <td>6</td>
            <td>1</td>
            <td>0.3459</td>
          </tr>
          <tr>
            <td>7</td>
            <td>2</td>
            <td>0.3585</td>
          </tr>
          <tr>
            <td>...</td>
            <td>...</td>
            <td>...</td>
          </tr>
          <tr>
            <td>Worst</td>
            <td>6</td>
            <td>0.4688</td>
          </tr>
        </table>
        <p class="figure-caption">
          Table 1: GLA layers ranked by normalized distillation loss. Lower loss
          indicates better suitability for replacement.
        </p>

        <p>
          The standout result is layer 3, which achieves a normalized loss of
          just 0.0003. This layer appears to perform computations that are
          nearly linear in nature, making it an ideal candidate for replacement.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The normalized loss is crucial for fair comparison. Without it, we would
        incorrectly conclude that layers with smaller activations are easier to
        replace.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The pattern of middle layers being hardest to replace aligns with
        findings from interpretability research showing that middle layers
        handle more abstract reasoning.
      </div>
    </div>

    <div class="content-margin-container" id="distillation">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>
          Stage 1: Full Model Distillation with Decoupled Top-k KL Divergence
        </h1>

        <p>
          After identifying which layers to replace, we assemble the hybrid
          model and fine-tune it using knowledge distillation from Qwen3-8B. We
          use a two-GPU configuration: the 8B teacher runs on GPU 0 (in FP16)
          while the 1.7B hybrid student runs on GPU 1.
        </p>

        <h2>Final Model Configuration</h2>
        <p>
          Based on Stage 0 results, our final hybrid model (top10_gla) replaces
          layers 1, 2, 3, 7, and 9 with GLA, keeping the remaining 23 layers as
          full attention. This configuration was chosen because these layers
          showed the lowest normalized distillation loss, indicating they could
          be well-approximated by linear attention.
        </p>

        <table class="results">
          <tr>
            <th>Configuration</th>
            <th>GLA Layers</th>
            <th>Full Attention Layers</th>
          </tr>
          <tr>
            <td>top10_gla (Final)</td>
            <td>[1, 2, 3, 7, 9]</td>
            <td>23</td>
          </tr>
        </table>
        <p class="figure-caption">Table 2: Final hybrid model configuration.</p>

        <h2>Decoupled Top-k KL Divergence</h2>
        <p>
          Standard KL divergence over the full vocabulary (150,000+ tokens) is
          dominated by low-probability tokens in the tail of the distribution.
          Following the approach from Liquid Foundation Models
          <a href="#ref_3">[3]</a>, we use a decoupled top-k KL divergence that
          focuses on the most important tokens.
        </p>

        <p>
          Let \(p_T\) and \(p_S\) denote the teacher and student probability
          distributions over the vocabulary \(\mathcal{V}\). For a given top-k
          set \(\mathcal{T}_k\) containing the k tokens with highest teacher
          probability, we define:
        </p>
        <p style="text-align: center">
          \[\rho_T = \sum_{v \in \mathcal{T}_k} p_T(v), \quad \rho_S = \sum_{v
          \in \mathcal{T}_k} p_S(v)\]
        </p>
        <p>
          These represent the total probability mass that each distribution
          assigns to the top-k tokens.
        </p>

        <p>
          The decoupled KL divergence consists of two components. First, a
          <strong>Bernoulli KL term</strong> that measures how well the student
          matches the teacher's allocation of probability mass between the top-k
          set and the rest:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{Bern}} = \rho_T \log\frac{\rho_T}{\rho_S} +
          (1-\rho_T) \log\frac{1-\rho_T}{1-\rho_S}\]
        </p>

        <p>
          Second, a <strong>categorical KL term</strong> computed only over the
          top-k tokens. We renormalize the distributions within the top-k set
          and compute KL divergence with temperature scaling:
        </p>
        <p style="text-align: center">
          \[\tilde{p}_T(v) = \frac{p_T(v)}{\rho_T}, \quad \tilde{p}_S(v) =
          \frac{p_S(v)}{\rho_S} \quad \text{for } v \in \mathcal{T}_k\]
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{top-k}} = T^2 \cdot \rho_T \cdot
          \text{KL}(\tilde{p}_T \| \tilde{p}_S)\]
        </p>
        <p>
          where \(T\) is the temperature parameter. The \(\rho_T\) factor
          weights this term by how much probability mass is in the top-k, and
          \(T^2\) compensates for gradient magnitude reduction from temperature
          scaling.
        </p>

        <p>The final KL loss combines both components:</p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{KL}} = \mathcal{L}_{\text{top-k}} +
          \mathcal{L}_{\text{Bern}}\]
        </p>

        <p>The total distillation loss is:</p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{total}} = \alpha \cdot \mathcal{L}_{\text{CE}} +
          (1-\alpha) \cdot \mathcal{L}_{\text{KL}}\]
        </p>
        <p>We use \(\alpha = 0.5\), \(T = 4.0\), and \(k = 32\) tokens.</p>

        <h2>Training Configuration</h2>
        <p>
          Stage 1 distillation uses batch size 1 with 8 gradient accumulation
          steps, sequence length 512, learning rate 5e-5 with cosine annealing,
          200 warmup steps, and 10,000 total training steps with BF16 precision
          and gradient checkpointing.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The Bernoulli term ensures the student allocates similar probability
        mass to the top-k set as the teacher. The categorical term ensures the
        relative ordering within top-k is preserved.
      </div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results</h1>

        <h2>Training Dynamics: Full KL vs Top-k KL</h2>
        <p>
          We compared training with full KL divergence versus our decoupled
          top-k KL divergence approach.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH LOSS CURVE OF TOTAL LOSS (WITH AND WITHOUT TOP-K)
          FIGURE]
        </div>
        <p class="figure-caption">
          Figure 2: Training loss curves comparing full KL divergence vs top-k
          KL divergence.
        </p>

        <p>
          <strong>Important:</strong> The total loss and KL loss values cannot
          be directly compared between the two methods because they operate on
          different scales. Full KL divergence sums over 150,000+ vocabulary
          items, while top-k KL operates on only 32 tokens. What we can compare
          is how well the student learns the language modeling task, as measured
          by the cross-entropy component. We observe that top-k KL divergence
          leads to better CE loss convergence, suggesting that focusing on
          high-probability tokens provides a cleaner learning signal.
        </p>

        <h2>Why Middle Layers Have the Highest Normalized Loss</h2>
        <p>
          Our Stage 0 results show that middle layers are significantly harder
          to replace than early or late layers. We interpret this as reflecting
          the functional organization of transformers:
        </p>

        <ul>
          <li>
            <strong>Early layers (1-7):</strong> Handle local patterns like
            syntax and n-grams. These can be captured by recurrent computations
            without requiring arbitrary-position attention.
          </li>
          <li>
            <strong>Middle layers (8-14):</strong> Perform complex reasoning
            including entity tracking, coreference resolution, and multi-hop
            inference. These tasks require attending to arbitrary positions
            based on semantic content.
          </li>
          <li>
            <strong>Late layers (15+):</strong> Aggregate information and
            project toward the output vocabulary. These computations may be more
            routine.
          </li>
        </ul>

        <h2>Throughput Comparison</h2>

        <div class="figure-placeholder">
          [FILL IN WITH GRAPH OF THROUGHPUT COMPARISONS FIGURE]
        </div>
        <p class="figure-caption">
          Figure 3: Throughput comparison across model configurations.
        </p>

        <table class="results">
          <tr>
            <th>Model</th>
            <th>GLA Layers</th>
            <th>Throughput (tok/s)</th>
            <th>Speedup</th>
            <th>Total Loss</th>
          </tr>
          <tr>
            <td>Qwen3-1.7B (baseline)</td>
            <td>0</td>
            <td>—</td>
            <td>1.00x</td>
            <td>—</td>
          </tr>
          <tr>
            <td>top10_gla Hybrid</td>
            <td>5</td>
            <td>—</td>
            <td>—</td>
            <td>—</td>
          </tr>
        </table>
        <p class="figure-caption">
          Table 3: Model configurations with throughput and loss metrics.
        </p>

        <h2>Qualitative Evaluation</h2>
        <p>To verify that the hybrid model maintains coherent generation:</p>

        <p>
          Unfortunately, our hybrid model did not achieve good performance in
          text prediction during qualitative evaluation. The generated outputs
          were often incoherent or failed to follow the prompt context
          appropriately.
        </p>

        <ul>
          <li>
            Due to limited training (only ~40 million tokens) and the challenges
            of distillation with replaced attention layers, the model produces
            poor quality outputs compared to the baseline.
          </li>
        </ul>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The scale difference between full and top-k KL makes direct loss
        comparison meaningless. The CE component is the fair comparison.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The middle-layer difficulty pattern was consistent across experiments,
        suggesting it reflects fundamental properties of the transformer
        architecture.
      </div>
    </div>

    <div class="content-margin-container" id="limitations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Limitations and Future Work</h1>

        <h2>Limitations</h2>
        <p>Our work has two primary limitations:</p>

        <p>
          <strong>Limited training compute:</strong> Our knowledge distillation
          training did not exceed approximately 40 million tokens (5,000 steps ×
          8 batch × 1,024 tokens ≈ 40,960,000 tokens). This is significantly
          less than the billions of tokens typically used for pretraining.
          Additionally, we observed that training time scaled superlinearly with
          layer depth, which limited our ability to train all layers within the
          available compute budget. With more compute and training tokens, the
          hybrid model would likely achieve better performance and more closely
          match the teacher model's capabilities.
        </p>

        <p>
          <strong>KV cache incompatibility:</strong> Standard transformer
          inference relies heavily on key-value (KV) caching to avoid
          recomputing attention for previous tokens. However, linear attention
          mechanisms like GLA use a fundamentally different computation paradigm
          based on recurrent state updates rather than explicit key-value
          storage. This means the standard KV cache approach does not directly
          apply to hybrid models. To achieve strong inference performance, we
          may need to implement a separate caching mechanism specifically
          designed for linear attention layers, which adds engineering
          complexity and may not provide the same speedups as traditional KV
          caching.
        </p>

        <h2>Future Work</h2>
        <p>Several directions could extend this work:</p>

        <ul>
          <li>
            <strong>Scaling training:</strong> Training with billions of tokens
            to better understand the efficiency-quality tradeoff at scale.
          </li>
          <li>
            <strong>Linear attention caching:</strong> Developing efficient
            caching mechanisms for linear attention that can work alongside
            traditional KV caches in hybrid models.
          </li>
          <li>
            <strong>Task-specific analysis:</strong> Measuring layer sensitivity
            for specific downstream tasks rather than just language modeling.
          </li>
        </ul>

        <h1>Conclusion</h1>
        <p>
          We presented a compute-aware architecture search method for
          constructing hybrid attention transformers. By training linear
          attention replacements for each layer independently and measuring
          normalized distillation loss, we can identify which layers are good
          candidates for replacement.
        </p>

        <p>
          Our key finding is that layer position strongly predicts amenability
          to linear attention replacement. Early layers can often be replaced
          with minimal quality loss, while middle layers are significantly
          harder to approximate. This suggests that middle layers perform more
          complex computations that fundamentally require the expressivity of
          softmax attention.
        </p>

        <p>
          We also adopted decoupled top-k KL divergence as an improved objective
          for knowledge distillation, focusing on high-probability tokens rather
          than matching the full distribution over 150,000+ vocabulary items.
        </p>

        <p>
          As language models continue to grow, efficient attention mechanisms
          become increasingly important. Our work provides a principled
          methodology for navigating the efficiency-quality tradeoff and
          contributes toward making large language models more practical to
          deploy.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The KV cache limitation is a significant practical concern for deploying
        hybrid models in production settings.
      </div>
    </div>

    <div class="content-margin-container" id="references">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="ref-section" style="height: auto">
          <br />
          <span style="font-size: 18px"><strong>References</strong></span
          ><br /><br />

          <a id="ref_1"></a>[1] Adler, B., et al. (2024).
          <a href="https://arxiv.org/abs/2402.16819"
            >Nemotron-4 15B Technical Report.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_2"></a>[2] Mercat, J., Vasiljevic, I., Keh, S., et al.
          (2024).
          <a href="https://arxiv.org/abs/2406.07588"
            >Linearizing Large Language Models.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_3"></a>[3] Liquid AI. (2024).
          <a href="https://arxiv.org/abs/2511.23404"
            >Liquid Foundation Models.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_4"></a>[4] Yang, S., Wang, B., Shen, Y., et al. (2024).
          <a href="https://arxiv.org/abs/2312.06635"
            >Gated Linear Attention Transformers with Hardware-Efficient
            Training.</a
          >
          ICML 2024.<br /><br />

          <a id="ref_5"></a>[5] Clark, K., Khandelwal, U., Levy, O., & Manning,
          C. D. (2019).
          <a href="https://arxiv.org/abs/1906.04341"
            >What Does BERT Look At? An Analysis of BERT's Attention.</a
          >
          BlackboxNLP 2019.<br /><br />

          <a id="ref_6"></a>[6] Tenney, I., Das, D., & Pavlick, E. (2019).
          <a href="https://arxiv.org/abs/1905.05950"
            >BERT Rediscovers the Classical NLP Pipeline.</a
          >
          ACL 2019.<br /><br />

          <a id="ref_7"></a>[7] Peng, B., et al. (2024).
          <a href="https://arxiv.org/abs/2404.05892"
            >Eagle and Finch: RWKV with Matrix-Valued States and Dynamic
            Recurrence.</a
          >
          ACL 2019.<br /><br />

          <a id="ref_7"></a>[7] Qwen Team. (2024).
          <a href="https://qwenlm.github.io/">Qwen2.5 Technical Report.</a
          ><br /><br />
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div
        class="main-content-block"
        style="text-align: center; padding: 30px; color: #666"
      >
        <hr />
        <p style="font-size: 14px">
          Code available at:
          <a href="https://github.com/your-repo/compute-aware-arch-search"
            >[GitHub Repository]</a
          >
        </p>
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>
</html>
