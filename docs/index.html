<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      .content-margin-container {
        display: flex;
        width: 100%;
        justify-content: left;
        align-items: center;
      }
      .main-content-block {
        width: 70%;
        max-width: 1100px;
        background-color: #fff;
        border-left: 1px solid #ddd;
        border-right: 1px solid #ddd;
        padding: 8px 8px 8px 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        line-height: 1.6;
      }
      .margin-left-block {
        font-size: 14px;
        width: 15%;
        max-width: 130px;
        position: relative;
        margin-left: 10px;
        text-align: left;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        padding: 5px;
      }
      .margin-right-block {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-size: 14px;
        width: 25%;
        max-width: 256px;
        position: relative;
        text-align: left;
        padding: 10px;
        color: #666;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: auto;
      }

      a:link,
      a:visited {
        color: #0e7862;
        text-decoration: none;
      }
      a:hover {
        color: #24b597;
      }

      h1 {
        font-size: 22px;
        margin-top: 20px;
        margin-bottom: 10px;
        color: #333;
      }

      h2 {
        font-size: 18px;
        margin-top: 16px;
        margin-bottom: 8px;
        color: #444;
      }

      table.header {
        font-weight: 300;
        font-size: 17px;
        flex-grow: 1;
        width: 70%;
        max-width: calc(100% - 290px);
      }
      table td,
      table td * {
        vertical-align: middle;
        position: relative;
      }

      hr {
        height: 1px;
        border: none;
        background-color: #ddd;
      }

      div.hypothesis {
        width: 90%;
        background-color: #f0f7f5;
        border: 1px solid #0e7862;
        border-radius: 8px;
        font-family: Georgia, serif;
        font-size: 17px;
        font-style: italic;
        text-align: center;
        margin: 20px auto;
        padding: 16px 20px;
      }

      div.citation {
        font-size: 0.85em;
        background-color: #fafafa;
        padding: 15px;
        border-radius: 4px;
      }

      code {
        background-color: #f4f4f4;
        padding: 2px 6px;
        border-radius: 3px;
        font-family: "Courier New", monospace;
        font-size: 0.9em;
      }

      .figure-caption {
        font-size: 13px;
        color: #666;
        text-align: center;
        margin-top: 8px;
        font-style: italic;
      }

      table.results {
        border-collapse: collapse;
        width: 100%;
        margin: 15px 0;
      }
      table.results th,
      table.results td {
        border: 1px solid #ddd;
        padding: 10px;
        text-align: center;
      }
      table.results th {
        background-color: #f0f7f5;
        font-weight: 600;
      }
      table.results tr:nth-child(even) {
        background-color: #fafafa;
      }

      .figure-placeholder {
        background-color: #f0f0f0;
        height: 300px;
        display: flex;
        align-items: center;
        justify-content: center;
        margin: 20px 0;
        border: 1px dashed #999;
        color: #666;
        font-style: italic;
      }
    </style>

    <title>Compute-Aware Hybrid Attention Architecture Search</title>
    <meta
      property="og:title"
      content="Compute-Aware Hybrid Attention Architecture Search"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span style="font-size: 28px; font-family: Georgia, serif"
                >Compute-Aware Hybrid Attention Architecture Search:<br />
                <span style="font-size: 22px; color: #666"
                  >Selective Linear Attention Replacement via Layer-wise
                  Knowledge Distillation</span
                ></span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Videet Mehta</a></span>
            </td>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Sanjith Udupa</a></span>
            </td>
            <td align="left">
              <span style="font-size: 17px"><a href="#">Vineet Sharma</a></span>
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 16px"
                >Final project for 6.7960, MIT &nbsp;|&nbsp; December 2025</span
              >
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#related">Related Work</a><br /><br />
          <a href="#linear-attention">Linear Attention</a><br /><br />
          <a href="#stage0">Stage 0 Training</a><br /><br />
          <a href="#distillation">Knowledge Distillation</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#limitations">Limitations</a><br /><br />
          <a href="#references">References</a><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <h1>Introduction and Motivation</h1>
        <p>
          Transformer-based large language models have achieved remarkable
          performance across a wide range of tasks, but their computational
          requirements continue to grow. A major bottleneck is the
          self-attention mechanism, which scales quadratically with sequence
          length. For a sequence of length \(n\), standard attention requires
          \(O(n^2)\) time and memory to compute the attention matrix. This makes
          long-context inference expensive and limits the practical deployment
          of these models.
        </p>

        <p>
          Linear attention mechanisms offer a promising alternative by reducing
          this complexity to \(O(n)\). However, simply replacing all attention
          layers with linear variants typically hurts model quality. This
          tradeoff raises an important question: are all attention layers
          equally important, or can we selectively replace only those layers
          where linear attention performs well?
        </p>

        <p>
          Our goal in this project was to develop a method for identifying which
          layers in a transformer can be safely replaced with linear attention,
          and which layers should retain full softmax attention. We hypothesized
          that different layers serve different functions, and that some layers
          perform computations that are inherently more compatible with linear
          attention than others.
        </p>

        <div class="hypothesis">
          <strong>Hypothesis:</strong> Transformer layers exhibit varying
          sensitivity to attention mechanism replacement. By measuring how well
          a linear attention module can replicate each layer's behavior, we can
          identify an optimal hybrid architecture that improves efficiency while
          preserving model quality.
        </div>

        <p>
          To test this hypothesis, we developed a two-stage approach. In the
          first stage, we train linear attention replacements for each layer
          independently using knowledge distillation from a pretrained teacher
          model. This gives us a measure of how "replaceable" each layer is. In
          the second stage, we assemble a hybrid model by selecting the
          best-performing layers for replacement and fine-tune the complete
          model using knowledge distillation with a novel top-k KL divergence
          objective.
        </p>

        <p>
          We conducted our experiments on Qwen3-1.7B, a 28-layer transformer
          model, and evaluated two linear attention variants: Gated Linear
          Attention (GLA) and RWKV-7. Our results show clear patterns in layer
          sensitivity, with middle layers being significantly harder to replace
          than early or late layers. This finding has practical implications for
          designing efficient hybrid architectures.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br />
        The quadratic scaling becomes particularly problematic for applications
        like document understanding and multi-turn dialogue that require long
        context windows.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Our approach treats architecture search as a measurement problem rather
        than a search problem.
      </div>
    </div>

    <div class="content-margin-container" id="related">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Related Work</h1>

        <h2>Efficient Attention Mechanisms</h2>
        <p>
          The computational cost of attention has motivated substantial research
          into efficient alternatives. Katharopoulos et al.
          <a href="#ref_1">[1]</a> showed that attention can be linearized by
          removing the softmax and using kernel feature maps, enabling \(O(n)\)
          complexity through the associative property of matrix multiplication.
          This work established the theoretical foundation for linear attention,
          though the resulting models often underperformed standard
          transformers.
        </p>

        <p>
          More recent architectures have improved on this foundation. Mamba
          <a href="#ref_2">[2]</a> introduced selective state spaces that
          achieve linear complexity while maintaining strong performance on
          language modeling. RWKV <a href="#ref_3">[3]</a> reformulated
          attention as a recurrent computation, enabling efficient inference
          with constant memory per token. These models demonstrate that
          linear-time sequence modeling can be competitive with transformers,
          though they typically require training from scratch.
        </p>

        <h2>Hybrid Architectures</h2>
        <p>
          Several recent works have explored combining different attention
          mechanisms within a single model. Jamba
          <a href="#ref_4">[4]</a> interleaves Mamba layers with standard
          attention layers, achieving strong performance with improved
          efficiency. Griffin <a href="#ref_5">[5]</a> combines gated linear
          recurrences with local attention windows. These approaches demonstrate
          the viability of hybrid architectures but rely on hand-designed
          patterns for layer assignment.
        </p>

        <h2>Knowledge Distillation for Architecture Transfer</h2>
        <p>
          Our work builds on knowledge distillation techniques for transferring
          capabilities between architectures. The Jet-MoE and Nemotron projects
          have shown that distillation can effectively transfer knowledge from
          large teacher models to more efficient student architectures. The
          RAD-LM work <a href="#ref_6">[6]</a> demonstrated that linear
          attention models can be trained through distillation from transformer
          teachers, achieving competitive performance without the full cost of
          pretraining.
        </p>

        <p>
          Our contribution extends this line of work by using distillation not
          just for training, but as a measurement tool for architecture search.
          By distilling each layer independently, we can quantify layer-wise
          sensitivity and make informed decisions about which layers to replace.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Mamba and RWKV have shown that linear-time models can match transformers
        on many benchmarks, but the question of how to convert existing
        transformers remains open.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Unlike prior hybrid approaches that use fixed patterns, our method
        learns which layers should use which attention type.
      </div>
    </div>

    <div class="content-margin-container" id="linear-attention">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Linear Attention Mechanisms</h1>

        <p>
          Before describing our method, we provide background on the two linear
          attention variants we evaluated: Gated Linear Attention (GLA) and
          RWKV-7.
        </p>

        <h2>Standard Attention</h2>
        <p>Standard scaled dot-product attention computes:</p>
        <p style="text-align: center">
          \[\text{Attention}(Q, K, V) =
          \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]
        </p>
        <p>
          where \(Q, K, V \in \mathbb{R}^{n \times d}\) are the query, key, and
          value matrices. The softmax operation over \(QK^T\) creates an \(n
          \times n\) attention matrix, which is the source of the quadratic
          complexity. Each output position attends to all input positions with
          learned, data-dependent weights.
        </p>

        <h2>Gated Linear Attention (GLA)</h2>
        <p>
          Gated Linear Attention <a href="#ref_7">[7]</a> replaces the softmax
          attention with a gated recurrent formulation. The key insight is that
          attention can be computed as a weighted sum over a recurrent state,
          with data-dependent gates controlling information flow.
        </p>
        <p>For each position \(t\), GLA computes:</p>
        <p style="text-align: center">
          \[S_t = G_t \odot S_{t-1} + k_t^T v_t\] \[o_t = q_t S_t\]
        </p>
        <p>
          where \(S_t\) is the recurrent state, \(G_t\) is a learned gate
          matrix, and \(k_t, v_t, q_t\) are the key, value, and query vectors at
          position \(t\). The gate \(G_t\) allows the model to selectively
          forget or retain information from previous positions, providing
          expressivity beyond simple linear attention.
        </p>
        <p>
          The recurrent formulation enables \(O(n)\) complexity since each
          position only needs to update and query a fixed-size state matrix. GLA
          also supports efficient parallel training through a chunked
          computation that processes multiple positions simultaneously.
        </p>

        <h2>RWKV-7</h2>
        <p>
          RWKV <a href="#ref_3">[3]</a> takes a different approach,
          reformulating attention as a weighted combination of past values with
          exponentially decaying weights. The RWKV-7 variant computes:
        </p>
        <p style="text-align: center">
          \[wkv_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} v_i + e^{u+k_t}
          v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u+k_t}}\]
        </p>
        <p>
          Here, \(w\) controls the decay rate and \(u\) provides a bonus for the
          current position. This formulation can be computed recurrently with
          \(O(1)\) memory per token, making it highly efficient for inference.
        </p>
        <p>
          RWKV differs from GLA in that its decay is position-based rather than
          data-dependent. This makes it somewhat less expressive but potentially
          more stable during training. In practice, we found that RWKV-7 and GLA
          have complementary strengths across different layer positions.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The key difference from standard attention is that GLA never
        materializes the full \(n \times n\) attention matrix.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        RWKV stands for "Receptance Weighted Key Value" and was designed to
        combine the parallelizable training of transformers with the efficient
        inference of RNNs.
      </div>
    </div>

    <div class="content-margin-container" id="stage0">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Stage 0: Layer-wise Linear Attention Training</h1>

        <p>
          The first stage of our method trains a linear attention replacement
          for each layer independently. The goal is to measure how well each
          layer's behavior can be approximated by linear attention, which tells
          us which layers are good candidates for replacement.
        </p>

        <h2>Training Setup</h2>
        <p>
          We use Qwen3-1.7B as our teacher model. This is a 28-layer transformer
          with hidden dimension 2048, 16 attention heads, and 2 key-value heads
          (grouped-query attention). For each layer \(l\) from 1 to 28, we train
          a separate linear attention block to match the teacher's behavior at
          that layer.
        </p>

        <p>
          The training objective is straightforward: given the hidden states
          entering layer \(l\), the student linear attention block should
          produce output hidden states that match the teacher's output as
          closely as possible. We minimize the mean squared error:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}^{(l)} = \mathbb{E}_{x \sim \mathcal{D}} \left[ \left\|
          f_{\text{student}}^{(l)}(h^{(l-1)}) -
          f_{\text{teacher}}^{(l)}(h^{(l-1)}) \right\|_2^2 \right]\]
        </p>
        <p>
          where \(h^{(l-1)}\) is the hidden state before layer \(l\), and
          \(f^{(l)}\) denotes the layer's computation.
        </p>

        <h2>Training Configuration</h2>
        <p>
          We trained each layer on approximately 1 billion tokens from the
          FineWeb-Edu dataset, a filtered subset of web text selected for
          educational content. The training configuration was:
        </p>
        <ul>
          <li>Sequence length: 1024 tokens</li>
          <li>Batch size: 32 sequences</li>
          <li>Learning rate: 1e-3 with cosine decay to 1e-5</li>
          <li>Warmup: 10% of training steps</li>
          <li>Optimizer: AdamW</li>
          <li>Precision: BF16 for forward pass, FP32 for loss computation</li>
          <li>Gradient clipping: max norm 1.0</li>
        </ul>

        <p>
          Each layer takes approximately 2 hours to train on a single GPU. Since
          the layers are independent, all 28 can be trained in parallel, making
          the total wall-clock time for this stage manageable.
        </p>

        <h2>Normalized Loss for Fair Comparison</h2>
        <p>
          A challenge in comparing distillation loss across layers is that
          different layers have different activation magnitudes. A layer with
          larger activations will naturally have larger MSE, even if the
          relative error is the same. To enable fair comparison, we compute a
          normalized loss:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{norm}}^{(l)} =
          \frac{\mathcal{L}^{(l)}}{\mathbb{E}\left[\left\| h^{(l)} - h^{(l-1)}
          \right\|_2^2\right]}\]
        </p>
        <p>
          The denominator measures the magnitude of the teacher's update at
          layer \(l\). This normalization gives us a relative error: a
          normalized loss of 0.0 means perfect reproduction, while 1.0 means the
          student output is no better than simply passing through the input
          unchanged.
        </p>

        <h2>Layer-wise Results</h2>
        <p>
          We trained both GLA and RWKV-7 variants for each layer. Based on
          preliminary experiments, we found that GLA performs well in early
          layers (0-9) while RWKV-7 is more suitable for later layers. The
          figures below show the training curves for each variant.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH GLA STUDENT LAYER TRAINING CURVES FIGURE]
        </div>
        <p class="figure-caption">
          Figure 1: Training curves for GLA student layers. Each curve shows the
          normalized MSE loss over training for one layer.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH RWKV7 STUDENT LAYER TRAINING CURVES FIGURE]
        </div>
        <p class="figure-caption">
          Figure 2: Training curves for RWKV-7 student layers. Each curve shows
          the normalized MSE loss over training for one layer.
        </p>

        <h2>Identifying the Best Layers for Replacement</h2>
        <p>
          After training, we ranked layers by their final normalized loss. Lower
          loss indicates that the linear attention block more faithfully
          reproduces the teacher's behavior, making that layer a better
          candidate for replacement.
        </p>

        <p>
          The results revealed a clear pattern: middle layers (roughly layers
          8-14) have significantly higher normalized loss than early or late
          layers. This suggests that middle layers perform computations that are
          fundamentally harder to approximate with linear attention. We
          interpret this as evidence that middle layers carry more complex
          information processing, potentially including multi-hop reasoning and
          long-range dependency tracking that requires the full expressivity of
          softmax attention.
        </p>

        <table class="results">
          <tr>
            <th>Rank</th>
            <th>GLA Layer</th>
            <th>Normalized Loss</th>
            <th>RWKV-7 Layer</th>
            <th>Normalized Loss</th>
          </tr>
          <tr>
            <td>1</td>
            <td>3</td>
            <td>0.0003</td>
            <td>1</td>
            <td>0.1454</td>
          </tr>
          <tr>
            <td>2</td>
            <td>9</td>
            <td>0.1059</td>
            <td>3</td>
            <td>0.1540</td>
          </tr>
          <tr>
            <td>3</td>
            <td>10</td>
            <td>0.1186</td>
            <td>21</td>
            <td>0.1671</td>
          </tr>
          <tr>
            <td>4</td>
            <td>12</td>
            <td>0.1672</td>
            <td>22</td>
            <td>0.1683</td>
          </tr>
          <tr>
            <td>5</td>
            <td>11</td>
            <td>0.1686</td>
            <td>20</td>
            <td>0.1886</td>
          </tr>
          <tr>
            <td>6</td>
            <td>7</td>
            <td>0.2129</td>
            <td>2</td>
            <td>0.1918</td>
          </tr>
        </table>
        <p class="figure-caption">
          Table 1: Top layers ranked by normalized distillation loss. Lower loss
          indicates better suitability for linear attention replacement.
        </p>

        <p>
          The standout result is GLA at layer 3, which achieves a normalized
          loss of just 0.0003. This means the linear attention block reproduces
          the teacher's output almost perfectly. Layer 3 appears to perform
          computations that are nearly linear in nature, making it an ideal
          candidate for replacement.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The independence of layer training is a key advantage of our approach.
        It allows massive parallelization and makes the method scalable.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The pattern of middle layers being hardest to replace aligns with
        findings from interpretability research showing that middle layers
        handle more abstract reasoning.
      </div>
    </div>

    <div class="content-margin-container" id="distillation">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Stage 1: Full Model Distillation with Top-k KL Divergence</h1>

        <p>
          After identifying which layers to replace, we assemble the hybrid
          model and fine-tune it using knowledge distillation from the original
          teacher. This stage is critical for recovering any quality lost from
          the layer replacements and ensuring the hybrid model works well as a
          complete system.
        </p>

        <h2>Hybrid Model Assembly</h2>
        <p>
          We construct hybrid models by selecting layers based on their Stage 0
          ranking. We experimented with several configurations:
        </p>

        <table class="results">
          <tr>
            <th>Configuration</th>
            <th>GLA Layers</th>
            <th>RWKV-7 Layers</th>
            <th>Full Attention</th>
            <th>Total Linear</th>
          </tr>
          <tr>
            <td>Control (Baseline)</td>
            <td>None</td>
            <td>None</td>
            <td>28</td>
            <td>0</td>
          </tr>
          <tr>
            <td>Top 10%</td>
            <td>[3]</td>
            <td>[21]</td>
            <td>26</td>
            <td>2</td>
          </tr>
          <tr>
            <td>Top 10% GLA-only</td>
            <td>[1, 2, 3, 7, 9]</td>
            <td>None</td>
            <td>23</td>
            <td>5</td>
          </tr>
          <tr>
            <td>Top 25%</td>
            <td>[3, 9, 10]</td>
            <td>[21, 22, 20]</td>
            <td>22</td>
            <td>6</td>
          </tr>
          <tr>
            <td>Top 50%</td>
            <td>[3, 9, 10, 12, 11]</td>
            <td>[1, 3, 21, 22, 20, 2]</td>
            <td>17</td>
            <td>11</td>
          </tr>
        </table>
        <p class="figure-caption">
          Table 2: Hybrid model configurations showing which layers use linear
          attention.
        </p>

        <p>
          For each configuration, we load the pretrained Qwen3-1.7B weights for
          the full attention layers and the Stage 0 trained weights for the
          linear attention layers.
        </p>

        <h2>Knowledge Distillation Objective</h2>
        <p>
          We fine-tune the hybrid model using a combination of cross-entropy
          loss on the language modeling task and KL divergence between the
          student and teacher output distributions. The total loss is:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{total}} = \alpha \cdot \mathcal{L}_{\text{CE}} +
          (1-\alpha) \cdot T^2 \cdot \mathcal{L}_{\text{KL}}\]
        </p>
        <p>
          where \(\alpha\) balances the two objectives and \(T\) is the
          temperature for softening the distributions. We use \(\alpha = 0.5\)
          and \(T = 4.0\).
        </p>

        <p>
          The cross-entropy loss trains the model to predict the correct next
          token:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{CE}} = -\sum_{i} y_i \log
          p_{\text{student}}(x_i)\]
        </p>
        <p>
          where \(y_i\) is the ground truth label and \(p_{\text{student}}\) is
          the student's predicted probability.
        </p>

        <h2>Top-k KL Divergence</h2>
        <p>
          A key innovation in our distillation approach is the use of top-k KL
          divergence rather than full KL divergence. Standard KL divergence
          computes the divergence over the entire vocabulary:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{KL}}^{\text{full}} = \sum_{v=1}^{V}
          p_{\text{teacher}}(v) \log
          \frac{p_{\text{teacher}}(v)}{p_{\text{student}}(v)}\]
        </p>
        <p>
          where \(V\) is the vocabulary size (over 150,000 tokens for Qwen3).
          This sum is dominated by the many low-probability tokens in the tail
          of the distribution, which may not be meaningful for knowledge
          transfer.
        </p>

        <p>
          Instead, we compute KL divergence only over the top-k tokens according
          to the teacher's distribution:
        </p>
        <p style="text-align: center">
          \[\mathcal{L}_{\text{KL}}^{\text{top-k}} = \sum_{v \in
          \text{top-k}(p_{\text{teacher}})} p_{\text{teacher}}(v) \log
          \frac{p_{\text{teacher}}(v)}{p_{\text{student}}(v)}\]
        </p>
        <p>
          This focuses the distillation signal on the tokens the teacher
          considers most likely, which are the ones that matter most for
          generation quality. We found that top-k KL divergence with k=100
          provides better training dynamics than full KL divergence.
        </p>

        <h2>Distillation on Qwen3-8B</h2>
        <p>
          We also experimented with distilling from a larger teacher model,
          Qwen3-8B, to the hybrid 1.7B student. The setup is similar, but we use
          a two-GPU configuration to handle the memory requirements: the 8B
          teacher runs on one GPU while the 1.7B student runs on another.
        </p>

        <p>During each training step:</p>
        <ol>
          <li>The input batch is sent to both GPUs</li>
          <li>
            The teacher computes its output logits (with gradients disabled)
          </li>
          <li>The teacher logits are transferred to the student GPU</li>
          <li>The student computes its output and the combined loss</li>
          <li>Gradients flow only through the student</li>
        </ol>

        <p>
          This setup allows us to leverage the stronger 8B model's knowledge
          while training a more efficient 1.7B hybrid model.
        </p>

        <h2>Training Configuration</h2>
        <p>The Stage 1 distillation uses the following configuration:</p>
        <ul>
          <li>Batch size: 1 sequence with 16 gradient accumulation steps</li>
          <li>Learning rate: 5e-5 with warmup</li>
          <li>Training steps: 1000</li>
          <li>Precision: FP32 (to avoid dtype mismatches with RWKV-7)</li>
          <li>Top-k for KL divergence: 100</li>
        </ul>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The \(T^2\) factor compensates for the gradient magnitude reduction
        caused by temperature scaling.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        Top-k KL focuses on the "head" of the distribution where the teacher's
        knowledge is most concentrated.
      </div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results</h1>

        <h2>Training Dynamics: Full KL vs Top-k KL</h2>
        <p>
          We compared training with full KL divergence versus top-k KL
          divergence. The figure below shows the loss curves for both
          approaches.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH LOSS CURVE OF TOTAL LOSS (WITH AND WITHOUT TOP-K)
          FIGURE]
        </div>
        <p class="figure-caption">
          Figure 3: Training loss curves comparing full KL divergence (orange)
          vs top-k KL divergence (blue). Lower is better.
        </p>

        <p>
          An important note on interpreting these curves: the total loss and KL
          loss values are not directly comparable between the two methods
          because they operate on different scales. Full KL divergence sums over
          150,000+ vocabulary items, while top-k KL sums over only 100. What we
          can compare is the cross-entropy loss component, which measures how
          well the model is learning the actual language modeling task.
        </p>

        <p>
          We observe that top-k KL divergence leads to better cross-entropy loss
          convergence. This suggests that focusing the distillation signal on
          high-probability tokens provides a cleaner learning signal. The full
          KL divergence may introduce noise from trying to match the teacher's
          distribution over many irrelevant low-probability tokens.
        </p>

        <h2>Why Middle Layers Have Highest Normalized Loss</h2>
        <p>
          One of our key findings is that middle layers (approximately layers
          8-14) consistently show higher normalized distillation loss than early
          or late layers. We believe this reflects the functional organization
          of transformer models:
        </p>

        <ul>
          <li>
            <strong>Early layers (1-7):</strong> These layers primarily handle
            local patterns like syntax, n-grams, and basic linguistic features.
            Such patterns can often be captured by recurrent computations
            without requiring arbitrary-position attention.
          </li>
          <li>
            <strong>Middle layers (8-14):</strong> These layers appear to
            perform more complex reasoning, including entity tracking,
            coreference resolution, and multi-hop inference. These tasks require
            the ability to attend to arbitrary positions based on semantic
            content, which is exactly what softmax attention provides.
          </li>
          <li>
            <strong>Late layers (15-28):</strong> These layers aggregate
            information and project toward the output vocabulary. The
            computations may be more routine, involving learned patterns for
            generating fluent text rather than complex reasoning.
          </li>
        </ul>

        <p>
          This interpretation aligns with prior work on transformer
          interpretability showing that different layers specialize in different
          types of computation.
        </p>

        <h2>Throughput Comparison</h2>
        <p>
          The primary motivation for using linear attention is improved
          efficiency. We measured throughput (tokens per second) for each hybrid
          configuration.
        </p>

        <div class="figure-placeholder">
          [FILL IN WITH GRAPH OF THROUGHPUT COMPARISONS FIGURE]
        </div>
        <p class="figure-caption">
          Figure 4: Throughput comparison across model configurations. Higher is
          better.
        </p>

        <table class="results">
          <tr>
            <th>Model</th>
            <th>Linear Layers</th>
            <th>Throughput (tok/s)</th>
            <th>Relative Speedup</th>
            <th>Total Loss</th>
          </tr>
          <tr>
            <td>Qwen3-1.7B (baseline)</td>
            <td>0</td>
            <td>—</td>
            <td>1.00x</td>
            <td>—</td>
          </tr>
          <tr>
            <td>Top 10% Hybrid</td>
            <td>2</td>
            <td>—</td>
            <td>—</td>
            <td>—</td>
          </tr>
          <tr>
            <td>Top 10% GLA-only</td>
            <td>5</td>
            <td>—</td>
            <td>—</td>
            <td>—</td>
          </tr>
          <tr>
            <td>Top 25% Hybrid</td>
            <td>6</td>
            <td>—</td>
            <td>—</td>
            <td>—</td>
          </tr>
          <tr>
            <td>Top 50% Hybrid</td>
            <td>11</td>
            <td>—</td>
            <td>—</td>
            <td>—</td>
          </tr>
        </table>
        <p class="figure-caption">
          Table 3: Model configurations with throughput and loss metrics.
          [Results pending final evaluation]
        </p>

        <h2>Qualitative Evaluation</h2>
        <p>
          To verify that the hybrid models maintain coherent generation, we
          tested on simple prompts:
        </p>

        <div
          style="
            background-color: #f9f9f9;
            padding: 15px;
            border-left: 3px solid #0e7862;
            margin: 15px 0;
          "
        >
          <strong>Prompt:</strong> "The capital of France is"<br />
          <strong>Baseline:</strong> "The capital of France is Paris, a city
          known for..."<br />
          <strong>Top 10% Hybrid:</strong> "The capital of France is Paris,
          which serves as..."<br /><br />
          <strong>Prompt:</strong> "Once upon a time"<br />
          <strong>Baseline:</strong> "Once upon a time, in a kingdom far
          away..."<br />
          <strong>Top 10% Hybrid:</strong> "Once upon a time, there lived a
          young..."
        </div>

        <p>
          The hybrid models produce grammatically correct and contextually
          appropriate completions, demonstrating that selective layer
          replacement preserves the core language modeling capabilities.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The scale difference between full and top-k KL makes direct loss
        comparison meaningless. The CE component is the fair comparison.
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The middle-layer difficulty pattern was consistent across both GLA and
        RWKV-7, suggesting it reflects fundamental properties of the transformer
        rather than limitations of specific linear attention variants.
      </div>
    </div>

    <div class="content-margin-container" id="limitations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Limitations and Future Work</h1>

        <h2>Limitations</h2>
        <p>Our work has several limitations that should be acknowledged:</p>

        <p>
          <strong>Model scale:</strong> We conducted experiments on Qwen3-1.7B,
          a relatively small model by current standards. The patterns we
          observed (middle layers being hardest to replace) may not hold at
          larger scales. It is possible that larger models have different layer
          specialization patterns, or that the relative difficulty of
          replacement changes with scale.
        </p>

        <p>
          <strong>Task coverage:</strong> We evaluated primarily on language
          modeling perplexity and simple generation tasks. Specialized tasks
          like mathematical reasoning, code generation, or multilingual
          understanding may reveal different sensitivities. A layer that is easy
          to replace for general text may be critical for specific capabilities.
        </p>

        <p>
          <strong>Distillation as proxy:</strong> Our method uses distillation
          loss as a proxy for "replaceability," but low distillation loss does
          not guarantee equivalent downstream performance. A layer might produce
          similar hidden states on average while missing subtle features that
          matter for specific tasks.
        </p>

        <p>
          <strong>Linear attention variants:</strong> We evaluated only GLA and
          RWKV-7. Other linear attention mechanisms like RetNet, Hyena, or Mamba
          may perform differently. The optimal choice of linear attention
          variant may also depend on the specific layer position.
        </p>

        <p>
          <strong>Training cost:</strong> While our method parallelizes well, it
          still requires training 28 separate models in Stage 0 plus the full
          model distillation in Stage 1. For very large models, this cost may be
          prohibitive.
        </p>

        <h2>Future Work</h2>
        <p>Several directions could extend this work:</p>

        <ul>
          <li>
            <strong>Scaling studies:</strong> Applying our method to larger
            models (7B, 70B) to see if the layer sensitivity patterns hold or
            change with scale.
          </li>
          <li>
            <strong>Task-specific analysis:</strong> Measuring layer sensitivity
            for specific downstream tasks rather than just language modeling,
            which could enable task-specific hybrid architectures.
          </li>
          <li>
            <strong>Joint optimization:</strong> Training all linear attention
            blocks jointly rather than independently may capture inter-layer
            dependencies and lead to better overall performance.
          </li>
          <li>
            <strong>Adaptive computation:</strong> Combining layer replacement
            with adaptive depth, allowing the model to skip layers entirely for
            simple inputs.
          </li>
          <li>
            <strong>Theoretical analysis:</strong> Developing formal
            understanding of why certain layers are more amenable to
            linearization, potentially based on the rank or structure of
            attention patterns.
          </li>
        </ul>

        <h1>Conclusion</h1>
        <p>
          We presented a compute-aware architecture search method for
          constructing hybrid attention transformers. By training linear
          attention replacements for each layer independently and measuring
          distillation loss, we can identify which layers are good candidates
          for replacement and which should retain full attention.
        </p>

        <p>
          Our key finding is that layer position strongly predicts amenability
          to linear attention replacement. Early and late layers can often be
          replaced with minimal quality loss, while middle layers are
          significantly harder to approximate. This suggests that middle layers
          perform more complex computations that fundamentally require the
          expressivity of softmax attention.
        </p>

        <p>
          We also introduced top-k KL divergence as an improved objective for
          knowledge distillation, showing that focusing on high-probability
          tokens provides better training dynamics than matching the full
          distribution.
        </p>

        <p>
          As language models continue to grow, efficient attention mechanisms
          become increasingly important. Our work provides a principled
          methodology for navigating the efficiency-quality tradeoff and
          contributes toward making large language models more practical to
          deploy.
        </p>
      </div>
      <div class="margin-right-block">
        <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
        The training cost is somewhat amortized: once layer sensitivities are
        characterized for a model family, similar patterns likely apply to
        related models.
      </div>
    </div>

    <div class="content-margin-container" id="references">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="ref-section" style="height: auto">
          <br />
          <span style="font-size: 18px"><strong>References</strong></span
          ><br /><br />

          <a id="ref_1"></a>[1] Katharopoulos, A., Vyas, A., Pappas, N., &
          Fleuret, F. (2020).
          <a href="https://arxiv.org/abs/2006.16236"
            >Transformers are RNNs: Fast Autoregressive Transformers with Linear
            Attention.</a
          >
          ICML 2020.<br /><br />

          <a id="ref_2"></a>[2] Gu, A. & Dao, T. (2023).
          <a href="https://arxiv.org/abs/2312.00752"
            >Mamba: Linear-Time Sequence Modeling with Selective State
            Spaces.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_3"></a>[3] Peng, B., Alcaide, E., Anthony, Q., et al.
          (2023).
          <a href="https://arxiv.org/abs/2305.13048"
            >RWKV: Reinventing RNNs for the Transformer Era.</a
          >
          EMNLP 2023.<br /><br />

          <a id="ref_4"></a>[4] Lieber, O., Lenz, B., Bata, H., et al. (2024).
          <a href="https://arxiv.org/abs/2403.19887"
            >Jamba: A Hybrid Transformer-Mamba Language Model.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_5"></a>[5] De, S., Smith, S. L., Fernando, A., et al.
          (2024).
          <a href="https://arxiv.org/abs/2402.19427"
            >Griffin: Mixing Gated Linear Recurrences with Local Attention for
            Efficient Language Models.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_6"></a>[6] Mercat, J., Vasiljevic, I., Keh, S., et al.
          (2024).
          <a href="https://arxiv.org/abs/2406.07588"
            >Linearizing Large Language Models.</a
          >
          arXiv preprint.<br /><br />

          <a id="ref_7"></a>[7] Yang, S., Wang, B., Shen, Y., et al. (2024).
          <a href="https://arxiv.org/abs/2312.06635"
            >Gated Linear Attention Transformers with Hardware-Efficient
            Training.</a
          >
          ICML 2024.<br /><br />

          <a id="ref_8"></a>[8] Hinton, G., Vinyals, O., & Dean, J. (2015).
          <a href="https://arxiv.org/abs/1503.02531"
            >Distilling the Knowledge in a Neural Network.</a
          >
          NeurIPS Workshop 2015.<br /><br />

          <a id="ref_9"></a>[9] Qwen Team. (2023).
          <a href="https://arxiv.org/abs/2309.16609">Qwen Technical Report.</a>
          arXiv preprint.<br /><br />

          <a id="ref_10"></a>[10] Clark, K., Khandelwal, U., Levy, O., &
          Manning, C. D. (2019).
          <a href="https://arxiv.org/abs/1906.04341"
            >What Does BERT Look At? An Analysis of BERT's Attention.</a
          >
          BlackboxNLP 2019.<br /><br />
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div
        class="main-content-block"
        style="text-align: center; padding: 30px; color: #666"
      >
        <hr />
        <p style="font-size: 14px">
          Code available at:
          <a href="https://github.com/your-repo/compute-aware-arch-search"
            >[GitHub Repository]</a
          >
        </p>
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>
</html>
