#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from collections.abc import Callable
from typing import Optional, Union

import torch
from torch import nn

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, DynamicCache
from transformers.generation import GenerationMixin
from transformers.integrations import use_kernel_forward_from_hub
from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_layers import (
    GenericForQuestionAnswering,
    GenericForSequenceClassification,
    GenericForTokenClassification,
    GradientCheckpointingLayer,
)
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple
from transformers.models.qwen3.configuration_qwen3 import Qwen3Config
from transformers import AutoConfig


@use_kernel_forward_from_hub("RMSNorm")
class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Qwen3RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config

        # Get rope_type from rope_scaling if it exists, otherwise default to "default"
        if config.rope_scaling is not None and "rope_type" in config.rope_scaling:
            self.rope_type = config.rope_scaling["rope_type"]
        else:
            self.rope_type = "default"
        
        rope_init_fn: Callable = self.compute_default_rope_parameters
        if self.rope_type != "default":
            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]
        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)

        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = inv_freq

    @staticmethod
    def compute_default_rope_parameters(
        config: Optional[Qwen3Config] = None,
        device: Optional["torch.device"] = None,
        seq_len: Optional[int] = None,
    ) -> tuple["torch.Tensor", float]:
        """
        Computes the inverse frequencies according to the original RoPE implementation
        Args:
            config ([`~transformers.PreTrainedConfig`]):
                The model configuration.
            device (`torch.device`):
                The device to use for initialization of the inverse frequencies.
            seq_len (`int`, *optional*):
                The current sequence length. Unused for this type of RoPE.
        Returns:
            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the
            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).
        """
        base = config.rope_theta
        dim = getattr(config, "head_dim", None) or config.hidden_size // config.num_attention_heads

        attention_factor = 1.0  # Unused in this type of RoPE

        # Compute the inverse frequencies
        inv_freq = 1.0 / (
            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)
        )
        return inv_freq, attention_factor

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.layer_type = config.layer_types[layer_idx] if hasattr(config, "layer_types") else None
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window if self.layer_type == "sliding_attention" else None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        # Save the input to the decoder block
        decoder_input = hidden_states
        
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # decoder_input = hidden_states
        # Self Attention
        pre_mlp_hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + pre_mlp_hidden_states

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output = self.mlp(hidden_states)
        decoder_output = residual + mlp_output
        # Return (input, output) of the decoder block
        return decoder_output, decoder_input


@auto_docstring
class Qwen3PreTrainedModel(PreTrainedModel):
    config: Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3DecoderLayer,
        "attentions": Qwen3Attention,
    }


@auto_docstring
class Qwen3Model(Qwen3PreTrainedModel):
    def __init__(self, config: Qwen3Config): #not trying to change the entire architecture, just adding the hoook. simpplifies the code
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.current_attention_hook_idx = getattr(config, "attention_hook_idx", None) # which index we are hooking at.
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)
        if self.current_attention_hook_idx is not None:
            num_layer = 0
            for decoder_layer in self.layers[: self.config.num_hidden_layers]:
                num_layer += 1
                decoder_output, decoder_input = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                    position_embeddings=position_embeddings,
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    **kwargs,
                )
                if num_layer == self.current_attention_hook_idx:
                    # Return (input to hook layer, output of hook layer)
                    # decoder_input is the input to this decoder block (output of previous layer)
                    # decoder_output is the output of this decoder block
                    return (decoder_input, decoder_output)
                hidden_states = decoder_output
        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            decoder_output, _ = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_embeddings=position_embeddings,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                **kwargs,
            )
            hidden_states = decoder_output

        hidden_states = self.norm(hidden_states)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
        )


@auto_docstring
class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # This line initializes the model's weights and performs any additional final setup required after model components are created.
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class Qwen3ForNAS(Qwen3ForCausalLM):
    def __init__(self, config):
        super().__init__(config)
    
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ):
        result = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs, #hook config should stay in here
        )
        if isinstance(result, tuple) and len(result) == 2:
            prev_hidden_states, hidden_states = result
            return prev_hidden_states, hidden_states
        print("result shape: ", result.shape)
        return result


# Constants for supported attention variants per layer
# Layer indices are 0-indexed (0-27 for 28 layers)
SUPPORTED_ATTENTION_VARIANTS = {
    "full_attention": list(range(28)),  # All layers 0-27
    "rwkv7": list(range(28)),  # All layers 0-27
    "gla": list(range(10)),  # Only layers 0-9
}


class Qwen3LinearAttentionDecoderLayer(GradientCheckpointingLayer):
    """Decoder layer that supports full attention, RWKV7, or GLA"""
    def __init__(
        self,
        config: Qwen3Config,
        layer_idx: int,
        attention_type: str = "full_attention",
        rwkv7_config=None,
        gla_config=None,
    ):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.attention_type = attention_type
        
        # MLP and layer norms are always the same
        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        # Choose attention mechanism
        if attention_type == "full_attention":
            self.attention = Qwen3Attention(config=config, layer_idx=layer_idx)
        elif attention_type == "rwkv7":
            if rwkv7_config is None:
                raise ValueError("rwkv7_config must be provided for RWKV7 attention")
            try:
                import warnings
                # Suppress FLA RWKV7 false positive warnings about tensor format
                # These warnings occur when seq_len < num_heads, which is normal for short sequences
                warnings.filterwarnings("ignore", message=".*Input tensor shape suggests potential format mismatch.*", category=UserWarning, module="fla.ops.rwkv7.fused_recurrent")
                from fla.models.rwkv7.modeling_rwkv7 import RWKV7Block
                from fla.layers.rwkv7 import RWKV7Attention
            except ImportError:
                raise ImportError("fla package not installed. Install with: pip install flash-linear-attention")
            # IMPORTANT: Use 1-based layer_idx to match training setup
            # Training uses layer_idx from range(1, num_layers + 1), so we convert 0-based to 1-based
            rwkv7_layer_idx = layer_idx + 1
            self.attention = RWKV7Block(config=rwkv7_config, layer_idx=rwkv7_layer_idx)
            
            # Patch RWKV7Attention to handle layer_idx=1 as first layer (for v_first computation)
            # RWKV7 internally checks layer_idx == 0, but training uses 1-based indices
            if hasattr(self.attention, 'attn') and isinstance(self.attention.attn, RWKV7Attention):
                original_forward = self.attention.attn.forward
                attn_instance = self.attention.attn
                
                def patched_forward(self_attn, hidden_states, attention_mask=None, past_key_values=None,
                                   use_cache=False, output_attentions=False, v_first=None, cu_seqlens=None, **kwargs):
                    # CRITICAL FIX: Convert v_first IMMEDIATELY and FORCE dtype match
                    # RWKV7 uses v_first in torch.lerp(v, v_first, ...) where v is in hidden_states.dtype
                    # During autocast (fp16 training), v will be float16 even if hidden_states is float32
                    # If v_first is float32 but v is fp16, torch.lerp will crash
                    # We MUST convert v_first BEFORE it's used, and ensure it's a new tensor (not in-place)
                    if v_first is not None:
                        # Determine target dtype: if autocast is enabled, use float16; otherwise match hidden_states
                        if torch.is_autocast_enabled():
                            # During mixed precision training, v will be float16 inside RWKV7
                            target_dtype = torch.float16
                        else:
                            # During fp32 training, match hidden_states dtype
                            target_dtype = hidden_states.dtype
                        
                        # CRITICAL: Must convert to match v's dtype (which will be float16 during autocast)
                        # Use .to() with explicit dtype and ensure it's actually converted
                        v_first = v_first.to(dtype=target_dtype)
                        # Double-check the conversion worked
                        assert v_first.dtype == target_dtype, f"v_first dtype conversion failed: expected {target_dtype}, got {v_first.dtype}"
                    
                    # Determine target dtype for return value (same as input conversion)
                    if torch.is_autocast_enabled():
                        return_dtype = torch.float16
                    else:
                        return_dtype = hidden_states.dtype
                    
                    # If this is the first RWKV7 layer (layer_idx=1 in 1-based) and v_first is None,
                    # temporarily set layer_idx to 0 so RWKV7 computes v_first internally
                    if v_first is None and self_attn.layer_idx == 1:
                        original_layer_idx = self_attn.layer_idx
                        self_attn.layer_idx = 0
                        try:
                            result = original_forward(hidden_states, attention_mask, past_key_values,
                                                     use_cache, output_attentions, v_first, cu_seqlens, **kwargs)
                        finally:
                            self_attn.layer_idx = original_layer_idx
                        # Ensure v_first in result matches the target dtype (float16 if autocast, else hidden_states.dtype)
                        if len(result) >= 4 and result[3] is not None:
                            result = (*result[:3], result[3].to(dtype=return_dtype), *result[4:])
                        return result
                    else:
                        # For non-first layers, v_first MUST be in correct dtype before lerp
                        # Pass the converted v_first to original_forward
                        result = original_forward(hidden_states, attention_mask, past_key_values,
                                               use_cache, output_attentions, v_first, cu_seqlens, **kwargs)
                        # Ensure v_first in result matches the target dtype (float16 if autocast, else hidden_states.dtype)
                        if len(result) >= 4 and result[3] is not None:
                            result = (*result[:3], result[3].to(dtype=return_dtype), *result[4:])
                        return result
                
                import types
                self.attention.attn.forward = types.MethodType(patched_forward, attn_instance)
        elif attention_type == "gla":
            if gla_config is None:
                raise ValueError("gla_config must be provided for GLA attention")
            try:
                from fla.models.gla.modeling_gla import GLABlock
            except ImportError:
                raise ImportError("fla package not installed. Install with: pip install flash-linear-attention")
            self.attention = GLABlock(config=gla_config, layer_idx=layer_idx)
        else:
            raise ValueError(f"Unsupported attention type: {attention_type}")
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,
        v_first: Optional[torch.Tensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ):
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        
        # Self Attention
        if self.attention_type == "full_attention":
            attn_output, _ = self.attention(
                hidden_states=hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )
            v_first_out = None
        elif self.attention_type == "rwkv7":
            # Match training setup: training doesn't pass past_key_values or use_cache
            # Convert 0-based layer_idx to 1-based for v_first logic (matching training)
            rwkv7_layer_idx = self.layer_idx + 1
            
            # Match training logic exactly:
            # - For layer_idx=0 (1-based = first RWKV7 layer): v_first=None (computed internally)
            # - For layer_idx != 0 (1-based): create dummy v_first with zeros
            # But if v_first is provided from a previous RWKV7 layer, use that instead
            if v_first is None:
                if rwkv7_layer_idx == 1:
                    # First RWKV7 layer: v_first=None (will be computed by RWKV7)
                    pass
                else:
                    # Not first RWKV7 layer: create dummy v_first like in training
                    batch_size, seq_len, _ = hidden_states.shape
                    # Get value_dim from config (same logic as training)
                    # Training uses: self.config.value_dim[self.layer_idx] if isinstance(self.config.value_dim, list) else (self.config.value_dim or self.config.hidden_size)
                    # where self.layer_idx is 1-based (1, 2, 3, ...)
                    if hasattr(self.attention, 'config'):
                        config = self.attention.config
                        # Match training exactly: use rwkv7_layer_idx (1-based) to index into value_dim list if it's a list
                        if isinstance(getattr(config, 'value_dim', None), list):
                            value_dim = config.value_dim[rwkv7_layer_idx]
                        else:
                            value_dim = getattr(config, 'value_dim', None) or config.hidden_size
                    else:
                        value_dim = hidden_states.shape[-1]
                    # Ensure v_first matches hidden_states dtype (important for fp16 training)
                    v_first = torch.zeros(batch_size, seq_len, value_dim, dtype=hidden_states.dtype, device=hidden_states.device)
            
            # CRITICAL: Convert v_first to match hidden_states dtype BEFORE passing to attention
            # RWKV7 uses v_first directly in torch.lerp(v, v_first, ...) and v is in hidden_states.dtype
            if v_first is not None:
                v_first = v_first.to(dtype=hidden_states.dtype)
            
            # Match training: only pass hidden_states, attention_mask, and v_first
            # Do NOT pass past_key_values or use_cache (they weren't used in training)
            attn_output, _, _, v_first_out = self.attention(
                hidden_states,
                attention_mask=attention_mask,
                v_first=v_first,
            )
            # Ensure v_first_out matches hidden_states dtype (RWKV7 may return it in float32)
            if v_first_out is not None:
                v_first_out = v_first_out.to(dtype=hidden_states.dtype)
        elif self.attention_type == "gla":
            # GLABlock returns (hidden_states, attentions, past_key_values, ...)
            # GLA doesn't use v_first
            attn_output, _, _, *_ = self.attention(
                hidden_states,
                attention_mask=attention_mask,
            )
            v_first_out = None
        else:
            raise ValueError(f"Unsupported attention type: {self.attention_type}")
        
        hidden_states = residual + attn_output
        
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output = self.mlp(hidden_states)
        hidden_states = residual + mlp_output
        
        return hidden_states, v_first_out


class Qwen3WithLinearAttentionModel(Qwen3PreTrainedModel):
    """Qwen3Model with configurable linear/full attention per layer"""
    def __init__(
        self,
        config: Qwen3Config,
        layer_attention_types: list[str],
        weights_base_path: Optional[str] = None,
        rwkv7_config=None,
        gla_config=None,
    ):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.layer_attention_types = layer_attention_types
        self.weights_base_path = weights_base_path
        
        # Validate layer attention types
        if len(layer_attention_types) != config.num_hidden_layers:
            raise ValueError(
                f"layer_attention_types must have length {config.num_hidden_layers}, "
                f"got {len(layer_attention_types)}"
            )
        
        for layer_idx, attn_type in enumerate(layer_attention_types):
            if attn_type not in SUPPORTED_ATTENTION_VARIANTS:
                raise ValueError(f"Unsupported attention type: {attn_type} at layer {layer_idx}")
            if layer_idx not in SUPPORTED_ATTENTION_VARIANTS[attn_type]:
                raise ValueError(
                    f"Attention type {attn_type} not supported for layer {layer_idx}. "
                    f"Supported layers: {SUPPORTED_ATTENTION_VARIANTS[attn_type]}"
                )
        
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList([
            Qwen3LinearAttentionDecoderLayer(
                config=config,
                layer_idx=layer_idx,
                attention_type=layer_attention_types[layer_idx],
                rwkv7_config=rwkv7_config,
                gla_config=gla_config,
            )
            for layer_idx in range(config.num_hidden_layers)
        ])
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        
        # Initialize weights and apply final processing
        self.post_init()
    
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # Prepare attention masks
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }

        hidden_states = inputs_embeds
        position_embeddings = self.rotary_emb(hidden_states, position_ids)
        
        # Find the first RWKV7 layer index (for v_first)
        first_rwkv7_idx = None
        for layer_idx, attn_type in enumerate(self.layer_attention_types):
            if attn_type == "rwkv7":
                first_rwkv7_idx = layer_idx
                break
        
        v_first = None
        for layer_idx, decoder_layer in enumerate(self.layers):
            if decoder_layer.attention_type == "full_attention":
                hidden_states, _ = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask_mapping["full_attention"],
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    **kwargs,
                )
            elif decoder_layer.attention_type == "rwkv7":
                # For the first RWKV7 layer, v_first is None (will be computed)
                # For subsequent RWKV7 layers, use v_first from the first one
                if layer_idx == first_rwkv7_idx:
                    current_v_first = None
                else:
                    # CRITICAL: Always convert v_first to match hidden_states dtype
                    # This must happen here to ensure dtype consistency before passing to decoder_layer
                    current_v_first = v_first.to(dtype=hidden_states.dtype) if v_first is not None else None
                
                # Match training: RWKV7 expects 2D attention_mask [batch_size, seq_len]
                # During generation, attention_mask might be None - create all-ones mask
                rwkv7_attention_mask = attention_mask
                if rwkv7_attention_mask is None:
                    # Create a simple all-ones mask matching hidden_states shape
                    batch_size, seq_len = hidden_states.shape[:2]
                    rwkv7_attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long, device=hidden_states.device)
                elif rwkv7_attention_mask.dim() != 2:
                    # If it's not 2D, try to extract or create 2D mask
                    if rwkv7_attention_mask.dim() == 4:
                        # 4D causal mask - extract 2D mask (all positions are valid)
                        rwkv7_attention_mask = torch.ones(hidden_states.shape[0], hidden_states.shape[1], dtype=torch.long, device=hidden_states.device)
                    else:
                        # Fallback: create all-ones mask
                        batch_size, seq_len = hidden_states.shape[:2]
                        rwkv7_attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long, device=hidden_states.device)
                
                hidden_states, v_first_out = decoder_layer(
                    hidden_states,
                    attention_mask=rwkv7_attention_mask,  # 2D mask for linear attention
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    v_first=current_v_first,
                    **kwargs,
                )
                
                # Store v_first from the first RWKV7 layer for subsequent layers
                # Ensure v_first matches hidden_states dtype (important for fp16 training)
                if layer_idx == first_rwkv7_idx and v_first_out is not None:
                    v_first = v_first_out.to(dtype=hidden_states.dtype)
            elif decoder_layer.attention_type == "gla":
                hidden_states, _ = decoder_layer(
                    hidden_states,
                    attention_mask=attention_mask,  # 2D mask for linear attention
                    position_ids=position_ids,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    **kwargs,
                )
            else:
                raise ValueError(f"Unsupported attention type: {decoder_layer.attention_type}")

        hidden_states = self.norm(hidden_states)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
        )


class Qwen3WithLinearAttention(Qwen3ForCausalLM):
    """Qwen3ForCausalLM with configurable linear/full attention per layer"""
    
    def __init__(
        self,
        config: Qwen3Config,
        layer_attention_types: list[str],
        base_model_path: Optional[str] = None,
        weights_base_path: Optional[str] = None,
        rwkv7_config=None,
        gla_config=None,
    ):
        # Temporarily store configs
        self._layer_attention_types = layer_attention_types
        self._base_model_path = base_model_path
        self._weights_base_path = weights_base_path
        self._rwkv7_config = rwkv7_config
        self._gla_config = gla_config
        
        # Call parent init but we'll replace the model
        super().__init__(config)
        
        # Replace the model with our mixed architecture model
        self.model = Qwen3WithLinearAttentionModel(
            config=config,
            layer_attention_types=layer_attention_types,
            weights_base_path=weights_base_path,
            rwkv7_config=rwkv7_config,
            gla_config=gla_config,
        )
        
        # Load weights
        self._load_weights(base_model_path, weights_base_path)
    
    def _load_weights(self, base_model_path: Optional[str], weights_base_path: Optional[str]):
        """Load weights from base model and trained linear attention checkpoints"""
        import os
        from safetensors.torch import load_file
        
        # First, load base model weights for all components
        if base_model_path is not None:
            base_model = Qwen3ForCausalLM.from_pretrained(
                base_model_path,
                config=self.config,
                torch_dtype=torch.float32,
            )
            base_state_dict = base_model.state_dict()
            student_state_dict = self.state_dict()
            
            # Copy weights from base model, excluding attention layers that will be replaced
            # Map base model keys to student model keys (self_attn -> attention)
            def map_key(key: str) -> str:
                """Map base model key to student model key"""
                # Replace self_attn with attention for full_attention layers
                if "model.layers." in key and ".self_attn." in key:
                    # Extract layer index
                    parts = key.split(".")
                    layer_idx = int(parts[2])
                    # Check if this layer uses full_attention
                    if layer_idx < len(self._layer_attention_types):
                        if self._layer_attention_types[layer_idx] == "full_attention":
                            # Map self_attn to attention
                            new_key = key.replace(".self_attn.", ".attention.")
                            return new_key
                return key
            
            loaded_count = 0
            skipped_count = 0
            shape_mismatch_count = 0
            for key, value in base_state_dict.items():
                # Map the key to student model key format
                student_key = map_key(key)
                
                # Check if this is an attention weight for a layer that uses linear attention
                is_linear_attn_weight = False
                if "model.layers." in key and ".attention." in key:
                    # Extract layer index from original key (before mapping)
                    parts = key.split(".")
                    layer_idx = int(parts[2])
                    if layer_idx < len(self._layer_attention_types):
                        if self._layer_attention_types[layer_idx] != "full_attention":
                            is_linear_attn_weight = True
                
                if not is_linear_attn_weight:
                    if student_key in student_state_dict:
                        if student_state_dict[student_key].shape == value.shape:
                            student_state_dict[student_key] = value
                            loaded_count += 1
                        else:
                            shape_mismatch_count += 1
                            if "attention" in key or "lm_head" in key or "embed" in key:
                                print(f"Shape mismatch for {key} -> {student_key}: base={value.shape}, student={student_state_dict[student_key].shape}")
                    elif key in student_state_dict:
                        # Try original key as fallback
                        if student_state_dict[key].shape == value.shape:
                            student_state_dict[key] = value
                            loaded_count += 1
                else:
                    skipped_count += 1
            
            self.load_state_dict(student_state_dict, strict=False)
            print(f"Loaded base model weights: {loaded_count} loaded, {skipped_count} skipped (linear attention), {shape_mismatch_count} shape mismatches")
        
        # Load trained linear attention weights
        if weights_base_path is not None:
            # Special case: use random weights for testing
            if weights_base_path == "TESTING_RANDOM":
                print("Using random weights for linear attention layers (TESTING_RANDOM mode)")
                for layer_idx, attn_type in enumerate(self._layer_attention_types):
                    if attn_type != "full_attention":
                        decoder_layer = self.model.layers[layer_idx]
                        # Initialize with random weights
                        for param in decoder_layer.attention.parameters():
                            if param.requires_grad:
                                torch.nn.init.normal_(param, mean=0.0, std=0.02)
                        print(f"Initialized {attn_type} with random weights for layer {layer_idx + 1}")
            else:
                # Normal case: load from files
                for layer_idx, attn_type in enumerate(self._layer_attention_types):
                    if attn_type != "full_attention":
                        # Construct path: weights_base_path/{attn_type}/safetensors/student_layer_{layer_idx+1}.safetensors
                        # Note: layer_idx is 0-indexed, but saved files use 1-indexed
                        safetensors_path = os.path.join(
                            weights_base_path,
                            attn_type,
                            "safetensors",
                            f"student_layer_{layer_idx + 1}.safetensors"
                        )
                        
                        if not os.path.exists(safetensors_path):
                            print(f"Warning: Linear attention weights not found: {safetensors_path}")
                            continue
                        
                        # Load weights
                        linear_weights = load_file(safetensors_path)
                        
                        # Get the decoder layer
                        decoder_layer = self.model.layers[layer_idx]
                        
                        # Map keys from safetensors (which have "decode_block." prefix) to model keys
                        mapped_weights = {}
                        for key, value in linear_weights.items():
                            # Remove "decode_block." prefix if present
                            if key.startswith("decode_block."):
                                new_key = key[len("decode_block."):]
                            else:
                                new_key = key
                            mapped_weights[new_key] = value
                        
                        # Load weights into the attention block
                        attention_state_dict = decoder_layer.attention.state_dict()
                        loaded_keys = []
                        for key, value in mapped_weights.items():
                            if key in attention_state_dict:
                                if attention_state_dict[key].shape == value.shape:
                                    attention_state_dict[key] = value
                                    loaded_keys.append(key)
                        
                        decoder_layer.attention.load_state_dict(attention_state_dict, strict=False)
                        print(f"Loaded {attn_type} weights for layer {layer_idx + 1} ({len(loaded_keys)} keys)")
    
    @classmethod
    def from_pretrained(
        cls,
        base_model_path: str,
        layer_attention_types: list[str],
        weights_base_path: Optional[str] = None,
        rwkv7_config_path: Optional[str] = None,
        gla_config_path: Optional[str] = None,
        **kwargs,
    ):
        """
        Create a Qwen3WithLinearAttention model from pretrained weights.
        
        Args:
            base_model_path: Path to base Qwen3 model
            layer_attention_types: List of attention types per layer (e.g., ["full_attention", "rwkv7", ...])
            weights_base_path: Base path for trained linear attention weights
                Expected structure: weights_base_path/{attn_type}/safetensors/student_layer_{idx}.safetensors
            rwkv7_config_path: Path to RWKV7 config JSON file
            gla_config_path: Path to GLA config JSON file
            **kwargs: Additional arguments passed to from_pretrained
        """
        import json
        
        # Load base config using Qwen3Config directly
        config = Qwen3Config.from_pretrained(base_model_path)
        
        # Load linear attention configs if needed
        rwkv7_config = None
        gla_config = None
        
        if "rwkv7" in layer_attention_types:
            if rwkv7_config_path is None:
                rwkv7_config_path = "linear_attn/rwkv7_config.json"
            with open(rwkv7_config_path, "r") as f:
                rwkv7_config_dict = json.load(f)
            from fla.models.rwkv7 import RWKV7Config
            rwkv7_config = RWKV7Config(**rwkv7_config_dict)
        
        if "gla" in layer_attention_types:
            if gla_config_path is None:
                gla_config_path = "linear_attn/gla_config.json"
            with open(gla_config_path, "r") as f:
                gla_config_dict = json.load(f)
            try:
                from fla.models.gla import GLAConfig
            except ImportError:
                raise ImportError("fla package not installed. Install with: pip install flash-linear-attention")
            gla_config = GLAConfig(**gla_config_dict)
        
        # Create model instance
        model = cls(
            config=config,
            layer_attention_types=layer_attention_types,
            base_model_path=base_model_path,
            weights_base_path=weights_base_path,
            rwkv7_config=rwkv7_config,
            gla_config=gla_config,
        )
        
        return model
    
    @classmethod
    def from_config_json(
        cls,
        config_path: str,
        **kwargs,
    ):
        """
        Create a Qwen3WithLinearAttention model from a JSON config file.
        
        Args:
            config_path: Path to JSON config file containing:
                - base_model_path: Path to base Qwen3 model
                - weights_base_path: Base path for trained linear attention weights (or "TESTING_RANDOM")
                - layer_attention_types: List of attention types per layer
                - rwkv7_config_path: (optional) Path to RWKV7 config JSON file
                - gla_config_path: (optional) Path to GLA config JSON file
            **kwargs: Additional arguments passed to from_pretrained
        """
        import json
        
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path, "r") as f:
            config_dict = json.load(f)
        
        # Extract required fields
        base_model_path = config_dict.get("base_model_path")
        if base_model_path is None:
            raise ValueError("base_model_path must be specified in config file")
        
        weights_base_path = config_dict.get("weights_base_path")
        layer_attention_types = config_dict.get("layer_attention_types")
        if layer_attention_types is None:
            raise ValueError("layer_attention_types must be specified in config file")
        
        if not isinstance(layer_attention_types, list):
            raise ValueError("layer_attention_types must be a list in the config file")
        
        # Optional config paths
        rwkv7_config_path = config_dict.get("rwkv7_config_path", "linear_attn/rwkv7_config.json")
        gla_config_path = config_dict.get("gla_config_path", "linear_attn/gla_config.json")
        
        # Use from_pretrained with extracted values
        return cls.from_pretrained(
            base_model_path=base_model_path,
            layer_attention_types=layer_attention_types,
            weights_base_path=weights_base_path,
            rwkv7_config_path=rwkv7_config_path,
            gla_config_path=gla_config_path,
            **kwargs,
        )


class Qwen3ForSequenceClassification(GenericForSequenceClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForTokenClassification(GenericForTokenClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForQuestionAnswering(GenericForQuestionAnswering, Qwen3PreTrainedModel):
    base_model_prefix = "transformer"  # For BC, where `transformer` was used instead of `model`


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3PreTrainedModel",
    "Qwen3Model",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
    "Qwen3ForNAS",
    "Qwen3WithLinearAttention",
    "SUPPORTED_ATTENTION_VARIANTS",
]